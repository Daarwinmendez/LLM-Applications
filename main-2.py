# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x1wL2lM2ryiRWAhwi_WZIa-BGwmCu_ZQ
"""

!pip install torch
!pip install tensorflow
!pip install evaluate
!pip install rouge_score
!pip install transformers
!pip install tf-keras
!pip install datasets
!pip install sacremoses==0.0.53
!pip install tiktoken
!pip install sentencepiece
!pip install einops
!pip install openpyxl
!pip install accelerate
!pip install seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use("ggplot")
import numpy as np
from datasets import load_dataset, list_metrics, load_metric
from sklearn.metrics import accuracy_score, confusion_matrix
from transformers import pipeline, set_seed, logging, AutoTokenizer
import tensorflow as tf
import keras as keras
import sentencepiece
import openpyxl
import os
from evaluate import load
import time
import torch
import warnings

"""## Estructura del Benchmark"""

# Crear MultiIndex para las columnas
columns = pd.MultiIndex.from_product(
    [["Summarization", "Text Classification", "Translation", "Text Generation", "Zero Shot Classification", "Few Shot Learning"],
     ["Name", "Inference Time", "Size (GB)", "Qty of Parameters", "Metric"]],
    names=["Task", "Info"]
)

# Crear MultiIndex para las filas (Modelos con subíndices de tamaño)
index = pd.MultiIndex.from_product(
    [["Model1", "Model2"], ["Size1", "Size2"]],
    names=["Model", "Size"]
)

# Crear el DataFrame vacío con el índice y las columnas definidas
benchmark = pd.DataFrame(columns=columns, index=index)
benchmark # Estructura del Benchmark



model_names = [],
model_n_params = []
model_sizes = []

#list_metrics()

"""## Creación de resumen (summarization)"""

cache_dir = "datasets"

xsum_dataset = load_dataset(
    "xsum", cache_dir=cache_dir,
    trust_remote_code=True

)

xsum_dataset # Descargo el set de datos



xsum_sample.iloc[0]["summary"]



pipeline?

"""### Set 1, modelo 1 (`T5`)

#### Small size
"""

small_summarizer = pipeline(
    task="summarization",
    model="t5-small",
    min_length=20,
    max_length=40,
    truncation=True,
    model_kwargs={"cache_dir": cache_dir}
)

model = small_summarizer.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")



# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

sample_for_summary = list(xsum_sample.head(30)["document"])
sample_for_summary

start_time = time.time()
small_summa_inference = pd.DataFrame(small_summarizer(sample_for_summary))
end_time = time.time()

time_taken = round(end_time - start_time, 4)
print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")

small_summa_inference

"""##### Evaluacion (Rouge)"""

def get_rouge_average(predictions, references):
    # Cargar la evaluación ROUGE
    rouge = load("rouge")

    # Calcular la puntuación ROUGE
    rouge_score = rouge.compute(predictions=predictions, references=references)

    # Calcular el promedio de las métricas ROUGE
    rouge_mean_on_test = np.mean(list(rouge_score.values()))

    return round(rouge_mean_on_test, 4)
# Definir las predicciones y referencias (en este caso, las primeras 15)
prediction = small_summa_inference["summary_text"].head(30)
reference = xsum_sample["summary"].head(30)

# Imprimir el promedio
rouge_mean_on_test = get_rouge_average(prediction, reference)
print(f"Promedio ROUGE en el test split: {rouge_mean_on_test:.4f}")

benchmark.loc[('Model1', "Size1"), ("Summarization", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Summarization", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Summarization", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size1"), ("Summarization", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size1"), ("Summarization", "Metric")] =  f"Rouge_average {rouge_mean_on_test}"

"""#### Large size"""

large_summarizer = pipeline(
    task="summarization",
    model="t5-large",
    max_length=40,
    min_length=20,
    truncation=False, # Si seteara Truncate a true, apareciera un error que dice: Asking to truncate to max_length
                    #but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
    model_kwargs={"cache_dir": cache_dir}
)

model = large_summarizer.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")


# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()
large_summa_inference = pd.DataFrame(large_summarizer(sample_for_summary))
end_time = time.time()

time_taken = round(end_time - start_time, 4)
print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")

large_summa_inference

"""##### Evaluación (Rouge Score)"""

prediction = large_summa_inference["summary_text"]
reference = xsum_sample["summary"].head(30)
rouge_mean_on_test = get_rouge_average(prediction, reference)
rouge_mean_on_test

benchmark.loc[('Model1', "Size2"), ("Summarization", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Summarization", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Summarization", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size2"), ("Summarization", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size2"), ("Summarization", "Metric")] =  f"Rouge_average {rouge_mean_on_test}"

"""### Set 1, modelo 2

#### Medium Size
"""

medium_summarizer_1 = pipeline(
    task="summarization",
    model="ainize/bart-base-cnn",
    min_length=20,
    max_length=40,
    truncation=True,
    model_kwargs={"cache_dir": cache_dir}
)

model = medium_summarizer_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

medium_summarizer_1?

"""##### Toma de tiempo"""

start_time = time.time()


sample_for_summary_bart = [text[:1024] for text in sample_for_summary]
#resumenes = [medium_summarizer_1(text, max_length=40, truncation=True) for text in sample_for_summary_bart]
medium_summa1_inference = pd.DataFrame(medium_summarizer_1(sample_for_summary_bart)) # Inferencia
end_time = time.time()
time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")

medium_summa1_inference

"""###### Evaluación (Rouge Score)"""

prediction = medium_summa1_inference["summary_text"]
reference = xsum_sample["summary"].head(30)
rouge_mean_on_test = get_rouge_average(prediction, reference)
rouge_mean_on_test

benchmark.loc[('Model2', "Size1"), ("Summarization", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Summarization", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Summarization", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size1"), ("Summarization", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size1"), ("Summarization", "Metric")] =  f"Rouge_average: {rouge_mean_on_test}"

"""#### Large Size"""

large_summarizer_1 = pipeline(
    task="summarization",
    model="facebook/bart-large-cnn",
    min_length=20,
    max_length=40,
    truncation=False,
    model_kwargs={"cache_dir": cache_dir}
)

model = large_summarizer_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")



start_time = time.time()

large_summa1_inference = pd.DataFrame(large_summarizer_1(sample_for_summary_bart, max_length=40))# Inferencia

end_time = time.time()
time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")

large_summa1_inference

prediction = large_summa1_inference["summary_text"]
reference = xsum_sample["summary"].head(30)
rouge_mean_on_test = get_rouge_average(prediction, reference)
rouge_mean_on_test

benchmark.loc[('Model2', "Size2"), ("Summarization", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Summarization", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Summarization", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size2"), ("Summarization", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size2"), ("Summarization", "Metric")] =  f"Rouge_average: {rouge_mean_on_test}"

benchmark["Summarization"]

"""## Clasificación de texto [para análisis de sentimiento]"""

poem_dataset = load_dataset(
    "poem_sentiment", cache_dir=cache_dir
)
poem_dataset

"""###### Lo que significa cada etiqueta
0 = negative

1 = positive

2 = no impact

3 = mixed (both negative and positive)

### Set 2, modelo 1

#### Medium Size (109 Millones de Parametros)
"""

sentiment_labels = {0: "negative", 1: "positive", 2: "no_impact",3: "mixed"} # etiquetas

poem_sample = poem_dataset["test"]
poem_sample = poem_sample.to_pandas().replace({"label": sentiment_labels})
poem_sample = poem_sample.reset_index(drop=True)
poem_sample

medium_sentiment_classifier = pipeline(
    task="text-classification",
    model="nickwong64/bert-base-uncased-poems-sentiment",
    model_kwargs={"cache_dir": cache_dir},
)

model = medium_sentiment_classifier.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

poem_sample["verse_text"]

start_time = time.time()

medium_sentiment_classifier_inferences = pd.DataFrame(medium_sentiment_classifier(list(poem_sample["verse_text"])))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
medium_sentiment_classifier_inferences

medium_sentiment_classifier_inferences["label"]

accuracy = accuracy_score(list(poem_sample["label"]), list(medium_sentiment_classifier_inferences["label"]))

classification_labels = poem_sample["label"].unique()
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample["label"], medium_sentiment_classifier_inferences["label"], labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Clasificación de sentimientos (LLMs)",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()



poem_sample["label"]
medium_sentiment_classifier_inferences["label"]

benchmark.loc[('Model1', "Size1"), ("Text Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Text Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Text Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size1"), ("Text Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size1"), ("Text Classification", "Metric")] =  f"Accuracy: {accuracy}"



"""#### Medium Size (355,361,794 Parametros)  """

large_sentiment_classifier = pipeline(
    task="text-classification",
    model="siebert/sentiment-roberta-large-english",
    model_kwargs={"cache_dir": cache_dir},
)

model = large_sentiment_classifier.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

poem_sample_large = poem_sample.loc[(poem_sample["label"] == "positive") | (poem_sample["label"] == "negative")]
poem_sample_large.reset_index(drop=True, inplace=True)

start_time = time.time()

large_sentiment_classifier_inference = pd.DataFrame(large_sentiment_classifier(list(poem_sample_large["verse_text"])))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
large_sentiment_classifier_inference



accuracy = accuracy_score(list(poem_sample_large["label"]), list(large_sentiment_classifier_inference["label"].apply(lambda x: x.lower())))

classification_labels = poem_sample_large["label"].unique()
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample_large["label"], large_sentiment_classifier_inference["label"].apply(lambda x: x.lower()), labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Clasificación de sentimientos (LLMs)",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model1', "Size2"), ("Text Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Text Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Text Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size2"), ("Text Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size2"), ("Text Classification", "Metric")] =  f"Accuracy: {accuracy}"

benchmark["Text Classification"]

"""### Set 2, modelo 2"""

glue_dataset = load_dataset(
    "nyu-mll/glue", cache_dir=cache_dir, name="mnli_matched"
)
glue_dataset

glue_sample = glue_dataset["validation"]
glue_sample = glue_sample.to_pandas().replace({"label":{0:"entailment", 1:"neutral",  2:"contradiction"}})
glue_sample = glue_sample.head(100)

glue_sample["premise"]

"""#### Small Size"""

small_sentiment_classifier_1 = pipeline(
    task="text-classification",
    model="WeightWatcher/albert-large-v2-mnli",
    padding=True,
    model_kwargs={"cache_dir": cache_dir},
)

model = small_sentiment_classifier_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

small_sentiment_classifier_1_inference =  small_sentiment_classifier_1(list(glue_sample.head(5)['premise']))

small_sentiment_classifier_1_inference

start_time = time.time()

small_sentiment_classifier_1_inference =  small_sentiment_classifier_1(list(glue_sample['hypothesis']))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_sentiment_classifier_1_inference = pd.DataFrame(small_sentiment_classifier_1_inference)
small_sentiment_classifier_1_inference

accuracy = accuracy_score(list(glue_sample["label"]), list(small_sentiment_classifier_1_inference["label"]))

classification_labels = glue_sample["label"].unique()
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(glue_sample["label"], list(small_sentiment_classifier_1_inference["label"]), labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Clasificación de sentimientos (LLMs)",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model2', "Size1"), ("Text Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Text Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Text Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size1"), ("Text Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size1"), ("Text Classification", "Metric")] =  f"Accuracy: {accuracy}"

"""#### Medium Size"""

medium_sentiment_classifier_1 = pipeline(
    task="text-classification",
    model="AiManatee/RoBERTa_poem_sentiment",
    model_kwargs={"cache_dir": cache_dir},
)

model = medium_sentiment_classifier_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

medium_sentiment_classifier_1_inference =  medium_sentiment_classifier_1(list(poem_sample["label"]))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
medium_sentiment_classifier_1_inference = pd.DataFrame(medium_sentiment_classifier_1_inference)
medium_sentiment_classifier_1_inference

medium_sentiment_classifier_1_inference["label"].unique()

accuracy = accuracy_score(list(poem_sample["label"]), list(medium_sentiment_classifier_1_inference["label"].apply(lambda x: x.lower())))

classification_labels = poem_sample["label"].unique()
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample["label"], medium_sentiment_classifier_1_inference["label"].apply(lambda x: x.lower()), labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Clasificación de sentimientos (LLMs)",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model2', "Size2"), ("Text Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Text Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Text Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size2"), ("Text Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size2"), ("Text Classification", "Metric")] =  f"Accuracy: {accuracy}"

benchmark["Text Classification"]

"""## Traducción"""

translation_dataset = load_dataset(
    "okezieowen/english_to_spanish", cache_dir=cache_dir,
)



translation_dataset

"""### Set 3, modelo 1"""

translation_dataset = translation_dataset["train"].to_pandas()

translation_dataset

sample_for_translation = translation_dataset.head(5)





"""#### Small Size"""

small_translation = pipeline(
    task="translation",
    #model="google-t5/t5-small",
    model="SEBIS/legal_t5_small_trans_es_en_small_finetuned",
    model_kwargs={"cache_dir": cache_dir},
)

model = small_translation.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

small_translation_inference =  small_translation(list(sample_for_translation["Spanish"]))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_translation_inference = pd.DataFrame(small_translation_inference)
small_translation_inference

small_translation_inference.iloc[0]["translation_text"]

# Cargar la métrica Bleu
bleu = load("bleu")

# Extraer las referencias de "English" desde sample_for_summary
reference_translations = [item for item in list(sample_for_translation["English"])]


# Calcular ROUGE asegurándote de que ambos son listas de texto
bleu = f"{bleu.compute(predictions=small_translation_inference["translation_text"], references=reference_translations)['bleu']:.4f}"

print("Bleu metric:", bleu)



benchmark.loc[('Model1', "Size1"), ("Translation", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Translation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Translation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size1"), ("Translation", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size1"), ("Translation", "Metric")] =  f"Bleu: {bleu}"

"""#### Medium Size"""

medium_translation = pipeline(
    task="translation",
    #model="google-t5/t5-base",
    model="zainnaved/marian-finetuned-kde4-en-to-es",
    model_kwargs={"cache_dir": cache_dir},
)

model = medium_translation .model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

medium_translation_inference =  medium_translation(list(sample_for_translation["English"]))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
medium_translation_inference = pd.DataFrame(medium_translation_inference)
medium_translation_inference

medium_translation_inference["translation_text"].iloc[0]

# Cargar la métrica Bleu
bleu = load("bleu")

# Extraer las referencias de "English" desde sample_for_summary
reference_translations = [item for item in list(sample_for_translation["Spanish"])]


# Calcular ROUGE asegurándote de que ambos son listas de texto
bleu = f"{bleu.compute(predictions=medium_translation_inference["translation_text"], references=reference_translations)['bleu']:.4f}"

print("Bleu metric:", bleu)

benchmark.loc[('Model1', "Size2"), ("Translation", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Translation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Translation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size2"), ("Translation", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size2"), ("Translation", "Metric")] =  f"Bleu: {bleu}"

benchmark["Translation"]

"""### Set 3, modelo 2

#### Small Size
"""

small_translation_1 = pipeline(
    task="translation",
    model="Helsinki-NLP/opus-mt-en-es",
    model_kwargs={"cache_dir": cache_dir},
)


model = small_translation_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

small_translation_1_inference = small_translation_1(list(sample_for_translation["English"]))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_translation_1_inference = pd.DataFrame(small_translation_1_inference)
small_translation_1_inference

small_translation_1_inference.iloc[0]["translation_text"]

# Cargar la métrica Bleu
bleu = load("bleu")

# Extraer las referencias de "English" desde sample_for_summary
reference_translations = [item for item in list(sample_for_translation["Spanish"])]


# Calcular ROUGE asegurándote de que ambos son listas de texto
bleu = f"{bleu.compute(predictions=small_translation_1_inference["translation_text"], references=reference_translations)['bleu']:.4f}"

print("Bleu metric:", bleu)

benchmark.loc[('Model2', "Size1"), ("Translation", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Translation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Translation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size1"), ("Translation", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size1"), ("Translation", "Metric")] =  f"Bleu: {bleu}"

benchmark["Translation"]

"""#### Large Size"""

translation_dataset_l1 = load_dataset(
    "seedboxai/german_to_english_translations_v1", cache_dir=cache_dir,
)

sample_for_translation_l1 = translation_dataset_l1["test"]
sample_for_translation_l1 = sample_for_translation_l1.to_pandas()


#### PENDING FOR RUNNING - THE ONE BELOW SHOULD BE THE REPLACEMENT

# Descargar solo las columnas "original" y "translation" del conjunto de prueba
translation_dataset_l1 = load_dataset(
    "seedboxai/german_to_english_translations_v1",
    split="test",
    cache_dir=cache_dir,
).select_columns(["original", "translation"])

# Convertir a DataFrame y renombrar las columnas
sample_for_translation_l1 = translation_dataset_l1.to_pandas()
sample_for_translation_l1.columns = ["English", "German"]

sample_for_translation_l1_ = sample_for_translation_l1.head(10)
sample_for_translation_l1_

large_translation_1 = pipeline(
    task="translation_en_to_de",
    model="google-t5/t5-base",
    max_length=500,
    model_kwargs={"cache_dir": cache_dir},

)

model = large_translation_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

large_translation_1_inference = pd.DataFrame(large_translation_1(sample_for_translation_l1_["English"].to_list()))
end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
large_translation_1_inference

large_translation_1_inference.iloc[0]["translation_text"]

# Cargar la métrica Bleu
bleu = load("bleu")

# Extraer las referencias de "English" desde sample_for_summary
reference_translations = [item for item in list(sample_for_translation_l1_["German"])]


# Calcular ROUGE asegurándote de que ambos son listas de texto
bleu = f"{bleu.compute(predictions=large_translation_1_inference["translation_text"], references=reference_translations)['bleu']:.4f}"

print("Bleu metric:", bleu)

benchmark.loc[('Model2', "Size2"), ("Translation", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Translation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Translation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size2"), ("Translation", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size2"), ("Translation", "Metric")] =  f"Bleu: {bleu}"

benchmark["Translation"]

"""## Generación de texto"""

incomplete_sentences = [ # 50 Oraciones Incompletas para dejar que el modelo genere texto.
    "The advancements in artificial intelligence are leading to",
    "In a world where technology continues to evolve, one day we might",
    "The future of humanity lies in the ability to merge",
    "When scientists discovered the new form of energy, they realized",
    "As I walked through the forest that day, I suddenly saw",
    "The secret to achieving true happiness is to",
    "With the discovery of life on another planet, scientists now",
    "After years of research, they finally created a device that can",
    "In the year 2050, the world has transformed into a place where",
    "Despite all the challenges, she believed that one day she would",
    "In the quiet of the evening, I heard a voice whisper",
    "The invention that changed the world forever was",
    "As they ventured into the unknown, the explorers discovered",
    "The book he left behind contained secrets about",
    "In the midst of chaos, she managed to find",
    "With great determination, he decided to pursue",
    "The ancient artifact was believed to hold the power to",
    "After analyzing the data, the scientist concluded that",
    "The unexpected letter he received revealed",
    "At the edge of the universe, there exists",
    "When I opened the mysterious box, I found",
    "The truth about her past was finally uncovered by",
    "As the rain poured down, I remembered",
    "In the heart of the city lies a hidden",
    "For generations, people have searched for",
    "In the silence of the night, a faint sound of",
    "The key to solving the mystery was hidden within",
    "In a world dominated by technology, humans have forgotten",
    "The hero stood before the crowd, ready to",
    "The strange object they discovered in the desert was",
    "During the experiment, the team noticed a strange",
    "The prophecy foretold that only one person could",
    "On the day of the celebration, everyone gathered to",
    "With newfound courage, she set out to",
    "In a remote village, people believe in the power of",
    "As they reached the summit of the mountain, they saw",
    "The purpose of the secret organization was to",
    "At the break of dawn, the soldiers prepared to",
    "Her dream was to create a world where everyone could",
    "After years of conflict, the two sides finally agreed to",
    "The sound of laughter filled the air as they began to",
    "For the first time in history, humans have the ability to",
    "The recipe handed down from her grandmother contained",
    "With only a map and a compass, he set out to find",
    "In the ancient library, she found a book that described",
    "They say the waterfall in the forest has the power to",
    "Her invention had the potential to change",
    "At the end of the journey, they discovered",
    "The artifact was a relic from a civilization that",
    "With a deep breath, he stepped forward and declared",
]

len(incomplete_sentences)

"""### Set 4, modelo 1

#### Small Size
"""

small_generator = pipeline(
    "text-generation", model="gpt2", # El mas pequeño de GPT2
    model_kwargs={"cache_dir": cache_dir},

    # Configuración de generación
    max_length=50,        # Longitud máxima del texto generado
    num_return_sequences=1, # Número de secuencias a generar
    do_sample=True,       # Usar muestreo probabilístico
    temperature=0.7,      # Control de creatividad
    top_k=50,            # Filtro de top-k tokens
    top_p=0.95,          # Nucleus sampling
    pad_token_id=50256,   # EOS token ID para GPT-2
    truncation=True

)

model = small_generator.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

small_generator_inference = [small_generator(sentence, truncation="longest_first", max_length=100)[0]["generated_text"] for sentence in incomplete_sentences]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_generator_inference

print(small_generator_inference[0]) # Viendo si tiene sentido.

perplexities = [] # Arreglo vacío para calcular perplexities.

for  sentence in incomplete_sentences:
    # Tokenizar el texto utilizando  el tokenizer del pipeline
    inputs = small_generator.tokenizer(sentence, return_tensors="pt") # return tensor for pytorch

    with torch.no_grad():
        outputs = small_generator.model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss

    # Calcular la perplejidad
    perplexity = torch.exp(loss)
    perplexities.append(perplexity)

perplexities = [p.item() for p in perplexities]
perplexities

df_small_gen_sentences_perpl = pd.DataFrame({"Generated Sentence": small_generator_inference, "Perplexity": perplexities})
df_small_gen_sentences_perpl

benchmark.loc[('Model1', "Size1"), ("Text Generation", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Text Generation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Text Generation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size1"), ("Text Generation", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size1"), ("Text Generation", "Metric")] =  f"Average perplexity: {np.mean(perplexities)}"

benchmark["Text Generation"]

"""#### Medium Size"""

medium_generator = pipeline(
    "text-generation", model="gpt2-medium", # El mediano de GPT2
    model_kwargs={"cache_dir": cache_dir},

    # Configuración de generación
    max_length=50,        # Longitud máxima del texto generado
    num_return_sequences=1, # Número de secuencias a generar
    do_sample=True,       # Usar muestreo probabilístico
    temperature=0.7,      # Control de creatividad
    top_k=50,            # Filtro de top-k tokens
    top_p=0.95,          # Nucleus sampling
    pad_token_id=50256,   # EOS token ID para GPT-2
    truncation=True
)

model = medium_generator.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

medium_generator_inference = [medium_generator(sentence, truncation="longest_first", max_length=100)[0]["generated_text"] for sentence in incomplete_sentences]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
medium_generator_inference

print(medium_generator_inference[0])

perplexities = [] # Arreglo vacío para calcular perplexities.

for  sentence in incomplete_sentences:
    # Tokenizar el texto utilizando  el tokenizer del pipeline
    inputs = medium_generator.tokenizer(sentence, return_tensors="pt") # return tensor for pytorch

    with torch.no_grad():
        outputs = medium_generator.model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss

    # Calcular la perplejidad
    perplexity = torch.exp(loss)
    perplexities.append(perplexity)

perplexities = [p.item() for p in perplexities]
perplexities

df_medium_gen_sentences_perpl = pd.DataFrame({"Generated Sentence": medium_generator_inference, "Perplexity": perplexities})
df_medium_gen_sentences_perpl

benchmark.loc[('Model1', "Size2"), ("Text Generation", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Text Generation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Text Generation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size2"), ("Text Generation", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size2"), ("Text Generation", "Metric")] =  f"Average perplexity: {np.mean(perplexities)}"

"""### Set 4, modelo 2

#### Small Size
"""

small_generator_1 = pipeline(
    task="text-generation",
    model="microsoft/DialoGPT-small",
    model_kwargs={"cache_dir": cache_dir},

    # Configuración de generación
    max_length=50,        # Longitud máxima del texto generado
    num_return_sequences=1, # Número de secuencias a generar
    do_sample=True,       # Usar muestreo probabilístico
    temperature=0.7,      # Control de creatividad
    top_k=50,            # Filtro de top-k tokens
    top_p=0.95,          # Nucleus sampling
    pad_token_id=50256,   # EOS token ID para GPT-2
    truncation=True
)

model = small_generator_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

small_generator_1_inference = [small_generator_1(sentence, truncation="longest_first", max_length=100)[0]["generated_text"] for sentence in incomplete_sentences]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_generator_1_inference

print(small_generator_1_inference[0])

perplexities = [] # Arreglo vacío para calcular perplexities.

for  sentence in incomplete_sentences:
    # Tokenizar el texto utilizando  el tokenizer del pipeline
    inputs = small_generator_1.tokenizer(sentence, return_tensors="pt") # return tensor for pytorch

    with torch.no_grad():
        outputs = small_generator_1.model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss

    # Calcular la perplejidad
    perplexity = torch.exp(loss)
    perplexities.append(perplexity)

perplexities = [p.item() for p in perplexities]
perplexities

df_small_1_gen_sentences_perpl = pd.DataFrame({"Generated Sentence": small_generator_1_inference, "Perplexity": perplexities})
df_small_1_gen_sentences_perpl

benchmark.loc[('Model2', "Size1"), ("Text Generation", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Text Generation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Text Generation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size1"), ("Text Generation", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size1"), ("Text Generation", "Metric")] =  f"Average perplexity: {np.mean(perplexities)}"

"""#### Large Size"""

large_generator_1 = pipeline(
    "text-generation", model="microsoft/DialoGPT-large", # 2.7 billones de parametros
    model_kwargs={"cache_dir": cache_dir},

    # Configuración de generación
    max_length=50,        # Longitud máxima del texto generado
    num_return_sequences=1, # Número de secuencias a generar
    do_sample=True,       # Usar muestreo probabilístico
    temperature=0.7,      # Control de creatividad
    top_k=50,            # Filtro de top-k tokens
    top_p=0.95,          # Nucleus sampling
    pad_token_id=50256,   # EOS token ID para GPT-2
    truncation=True
)

model = large_generator_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

large_generator_1_inference = [large_generator_1(sentence, truncation="longest_first", max_length=100)[0]["generated_text"] for sentence in incomplete_sentences]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
large_generator_1_inference

print(large_generator_1_inference[0])

perplexities = [] # Arreglo vacío para calcular perplexities.

for  sentence in incomplete_sentences:
    # Tokenizar el texto utilizando  el tokenizer del pipeline
    inputs = large_generator_1.tokenizer(sentence, return_tensors="pt") # return tensor for pytorch

    with torch.no_grad():
        outputs = large_generator_1.model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss

    # Calcular la perplejidad
    perplexity = torch.exp(loss)
    perplexities.append(perplexity)

perplexities = [p.item() for p in perplexities]
perplexities

df_large_1_gen_sentences_perpl = pd.DataFrame({"Generated Sentence": large_generator_1_inference, "Perplexity": perplexities})
df_large_1_gen_sentences_perpl

benchmark.loc[('Model2', "Size2"), ("Text Generation", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Text Generation", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Text Generation", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size2"), ("Text Generation", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size2"), ("Text Generation", "Metric")] =  f"Average perplexity: {np.mean(perplexities)}"

benchmark["Text Generation"]

"""## Zero-shot classification"""

poem_sample

poem_sample["label"].unique()

poem = poem_sample["verse_text"].iloc[0]
poem

def categorize_poem(modelo, poem: str) -> str:
    """
    Define categorías de clasificación y devuelve la categoría con la mayor confianza.

    Arguments:
        poem (str): El poema que será clasificado.

    Returns:
        str: La etiqueta más probable con mayor puntuación.

    """
    results = modelo(
        poem,
        candidate_labels=[
            'no_impact',
            'positive',
            'negative',
        ]
    )

    return results["labels"][np.argmax(results["scores"])] # Retorna la etiqueta cuyo score es mas alto, en el dominio de los arreglos

categorize_poem(small_zero_shot, poem)

"""### Set 5, modelo 1

#### Small Size
"""

def unpack_tuple(dataFrame):
    unpacked = [];
    for val in dataFrame:
        unpacked.append(*val)
    return unpacked

# Crear pipeline de clasificación de cero disparos
small_zero_shot = pipeline(
    task="zero-shot-classification",
    model="cross-encoder/nli-deberta-v3-small",
    model_kwargs={"cache_dir": cache_dir},
)
model = small_zero_shot.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

small_zero_shot_inferences = [categorize_poem(small_zero_shot, poem) for poem in poem_sample["verse_text"]]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
small_zero_shot_inferences

print(small_zero_shot_inferences[-1],"\n", poem_sample["verse_text"].iloc[-1])

accuracy = accuracy_score(list(poem_sample["label"]), small_zero_shot_inferences)

classification_labels = unpack_tuple(pd.DataFrame(small_zero_shot_inferences).value_counts().keys().to_list())
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample["label"], small_zero_shot_inferences, labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Zero Shot Classification",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model1', "Size1"), ("Zero Shot Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Zero Shot Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Zero Shot Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size1"), ("Zero Shot Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size1"), ("Zero Shot Classification", "Metric")] =  f"Accuracy: {accuracy}"

"""#### Medium Size"""

zero_shot_base = pipeline(
    task="zero-shot-classification",
    model='cross-encoder/nli-deberta-base',
    model_kwargs={"cache_dir": cache_dir})

model = zero_shot_base.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

zero_shot_base_inferences = [categorize_poem(zero_shot_base, poem) for poem in poem_sample["verse_text"]]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
zero_shot_base_inferences

print(zero_shot_base_inferences[-1],"\n", poem_sample["verse_text"].iloc[-1])

accuracy = accuracy_score(list(poem_sample["label"]), zero_shot_base_inferences)

classification_labels = unpack_tuple(pd.DataFrame(zero_shot_base_inferences).value_counts().keys().to_list())
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample["label"], zero_shot_base_inferences, labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Zero Shot Classification",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model1', "Size2"), ("Zero Shot Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Zero Shot Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Zero Shot Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model1', "Size2"), ("Zero Shot Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model1', "Size2"), ("Zero Shot Classification", "Metric")] =  f"Accuracy: {accuracy}"

"""### Set 5, modelo 2

#### Medium Size
"""

zero_shot_base_1 = pipeline(
    task="zero-shot-classification",
    model='mjwong/multilingual-e5-base-xnli',
    model_kwargs={"cache_dir": cache_dir})


model = zero_shot_base_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

zero_shot_base_1_inferences = [categorize_poem(zero_shot_base_1, poem) for poem in poem_sample["verse_text"]]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
zero_shot_base_1_inferences

print(zero_shot_base_1_inferences[-1],"\n", poem_sample["verse_text"].iloc[-1])

accuracy = accuracy_score(list(poem_sample["label"]), zero_shot_base_1_inferences)

classification_labels = unpack_tuple(pd.DataFrame(zero_shot_base_1_inferences).value_counts().keys().to_list())
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
conf_mat = confusion_matrix(poem_sample["label"], zero_shot_base_1_inferences, labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Zero Shot Classification",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

benchmark.loc[('Model2', "Size1"), ("Zero Shot Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Zero Shot Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Zero Shot Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size1"), ("Zero Shot Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size1"), ("Zero Shot Classification", "Metric")] =  f"Accuracy: {accuracy}"

"""#### Large Size"""

zero_shot_large_1 = pipeline(
    task="zero-shot-classification",
    model='mjwong/multilingual-e5-large-xnli',
    model_kwargs={"cache_dir": cache_dir})


model = zero_shot_large_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

start_time = time.time()

zero_shot_large_1_inferences = [categorize_poem(zero_shot_large_1, poem) for poem in poem_sample["verse_text"]]

end_time = time.time()

time_taken = round(end_time - start_time, 4)

print(f"Tiempo tomado para inferenciar: {time_taken} Segundos...")
zero_shot_large_1_inferences

print(zero_shot_large_1_inferences[-1],"\n", poem_sample["verse_text"].iloc[-1])

accuracy = accuracy_score(list(poem_sample["label"]), zero_shot_large_1_inferences)

#classification_labels = poem_sample["label"].unique()
classification_labels = unpack_tuple(pd.DataFrame(zero_shot_large_1_inferences).value_counts().keys().to_list())
print(f"accuracy: {accuracy}")

fig, ax =  plt.subplots(figsize=(7, 5))
#zero_shot_large_1_inferences
conf_mat = confusion_matrix(poem_sample["label"], zero_shot_large_1_inferences, labels=classification_labels)

ax = sns.heatmap(conf_mat, annot=True, xticklabels=classification_labels, yticklabels=classification_labels, cmap="Blues")

ax.set(
    title="Matriz de confusión Para Zero Shot Classification",
    xlabel="Valor Predicho",
    ylabel="Valor Verdadero"
)
plt.tight_layout()
plt.show()

print(len([inference for inference in zero_shot_large_1_inferences if inference=="negative"]))
print(len([inference for inference in zero_shot_large_1_inferences if inference=="positive"]))
print(len([inference for inference in zero_shot_large_1_inferences if inference=="no_impact"]))

benchmark.loc[('Model2', "Size2"), ("Zero Shot Classification", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Zero Shot Classification", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Zero Shot Classification", "Qty of Parameters")] =  total_params
benchmark.loc[('Model2', "Size2"), ("Zero Shot Classification", "Inference Time")] =  time_taken
benchmark.loc[('Model2', "Size2"), ("Zero Shot Classification", "Metric")] =  f"Accuracy: {accuracy}"

benchmark["Zero Shot Classification"]

"""## Few-shot learning [prompt engineering] [text gen]

### Set 6, modelo 1

#### Medium Size
"""

# We will limit the response length for our few-shot learning tasks.
few_shot_medium = pipeline(
    task="text-generation",
    model="EleutherAI/gpt-neo-125m",
    max_new_tokens=40,
    eos_token_id= few_shot_medium.tokenizer.eos_token_id,
    model_kwargs={"cache_dir": cache_dir},
)

model = few_shot_medium.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

import warnings
warnings.filterwarnings("ignore")

# Arreglo de poemas a clasificar
poemas = [
    "The sun rises over the horizon",
    "The storm clouds darken the sky",
    "A new dawn brings hope",
    "Rage filled the hearts of the warriors",
    "The stars shine brightly in the night"
]

# Prompt base con ejemplos etiquetados
prompt_base = """For each poem, describe its sentiment as Positive, Negative, or Neutral.

[Poem]: "The sun rises over the horizon"
[Sentiment]: Positive
###
[Poem]: "The storm clouds darken the sky"
[Sentiment]: Negative
###
[Poem]: "A new dawn brings hope"
[Sentiment]: Positive
###
[Poem]: "Rage filled the hearts of the warriors"
[Sentiment]: Negative
###
[Poem]: "The stars shine brightly in the night"
[Sentiment]: Positive
###
"""

# Función para obtener el sentimiento de cada poema
def obtener_sentimiento(poema):
    prompt = prompt_base + f'[Poem]: "{poema}"\n[Sentiment]:'

    # Genera la predicción
    result = few_shot_medium(
        prompt,
        max_new_tokens=10,  # Ajusta la longitud de texto generado
        num_return_sequences=1,
        do_sample=True,
        temperature=0.7
    )

    # Postprocesamiento para extraer solo el sentimiento (Positive, Negative o Neutral)
    generated_text = result[0]["generated_text"]
    for sentimiento in ["Positive", "Negative", "Neutral"]:
        if sentimiento in generated_text:
            return sentimiento
    return "Unknown"  # Por si no se detecta ningún sentimiento válido

# Recorre el arreglo de poemas y obtiene el sentimiento para cada uno
for poema in poemas:
    sentimiento = obtener_sentimiento(poema)
    print(f'Poema: "{poema}"\nSentimiento: {sentimiento}\n')

sentiment_inference_df



benchmark.loc[('Model1', "Size1"), ("Few Shot Learning", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size1"), ("Few Shot Learning", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size1"), ("Few Shot Learning", "Qty of Parameters")] =  total_params

"""#### Large Size"""

# We will limit the response length for our few-shot learning tasks.
few_shot_large = pipeline(
    task="text-generation",
    model="EleutherAI/gpt-neo-1.3B",
    max_new_tokens=10,
    model_kwargs={"cache_dir": cache_dir},
)

model = few_shot_large.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

benchmark.loc[('Model1', "Size2"), ("Few Shot Learning", "Name")] =  model.name_or_path
benchmark.loc[('Model1', "Size2"), ("Few Shot Learning", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model1', "Size2"), ("Few Shot Learning", "Qty of Parameters")] =  total_params

eos_token_id = few_shot_large.tokenizer.encode("###")[0]

# Without any examples, the model output is inconsistent and usually incorrect.
    # With 1 example for each sentiment, the model is more likely to understand!
    results = few_shot_large(
        """For each tweet, describe its sentiment:

    [Tweet]: "I hate it when my phone battery dies."
    [Sentiment]: Negative
    ###
    [Tweet]: "My day has been 👍"
    [Sentiment]: Positive
    ###
    [Tweet]: "This is the link to the article"
    [Sentiment]: Neutral
    ###
    [Tweet]: "This new music video was incredible"
    [Sentiment]:""",
        eos_token_id=eos_token_id,
    )

    print(results[0]["generated_text"])

"""### Set 6, modelo 2

#### Medium Size
"""

# We will limit the response length for our few-shot learning tasks.
few_shot_medium_1 = pipeline(
    task="text-generation",
    model="microsoft/biogpt",
    max_new_tokens=10,
    model_kwargs={"cache_dir": cache_dir},
)

model = few_shot_medium_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")

# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

benchmark.loc[('Model2', "Size1"), ("Few Shot Learning", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size1"), ("Few Shot Learning", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size1"), ("Few Shot Learning", "Qty of Parameters")] =  total_params



"""#### Large Size"""

# We will limit the response length for our few-shot learning tasks.
few_shot_large_1 = pipeline(
    task="text-generation",
    model="microsoft/BioGPT-Large",
    max_new_tokens=10,
    model_kwargs={"cache_dir": cache_dir},
)

model = few_shot_large_1.model
total_params = sum(p.numel() for p in model.parameters())
print(f"El modelo tiene aproximadamente {total_params:,} parámetros.")


# Calcular el tamaño en memoria del modelo en base al dtype de los parámetros
model_memory_size = sum(p.numel() * p.element_size() for p in model.parameters())
model_memory_size_gb = model_memory_size / (1024 ** 3)  # Convertir a GB

# Mostrar el tamaño en memoria
print(f"El modelo tiene aproximadamente {model_memory_size_gb:.2f} GB en memoria.")

benchmark.loc[('Model2', "Size2"), ("Few Shot Learning", "Name")] =  model.name_or_path
benchmark.loc[('Model2', "Size2"), ("Few Shot Learning", "Size (GB)")] =  model_memory_size_gb
benchmark.loc[('Model2', "Size2"), ("Few Shot Learning", "Qty of Parameters")] =  total_params

benchmark

benchmark.to_excel("benchmark.xlsx")

(benchmark.T).to_excel("benchmark_v2.xlsx")

